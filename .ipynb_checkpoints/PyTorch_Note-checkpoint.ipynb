{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial\n",
    "\n",
    "link: https://github.com/yunjey/pytorch-tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA是否可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "# GPU可以使用则使用GPU\n",
    "\n",
    "tensor = torch.tensor([1,2])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor.to('gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor概念："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "data = [[1,2],[3,4]]\n",
    "x = torch.tensor(data)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor from numpy\n",
    "data = np.arange(6).reshape(2,3)\n",
    "x = torch.from_numpy(data)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6322, 0.0205, 0.1351],\n",
       "        [0.0851, 0.4695, 0.0295]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从别的tensor导入\n",
    "eta = torch.ones_like(x)\n",
    "eta2 = torch.rand_like(x, dtype = torch.float)  # 两者是不同类型的需要进行类型指定\n",
    "print(eta)\n",
    "eta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.6855, 0.6207, 0.2093],\n",
      "        [0.4596, 0.6651, 0.2370]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([6, 7, 8, 9]) \n",
      "\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建不同类型的特殊tensor\n",
    "shape = (2,3)\n",
    "a = torch.ones(shape)\n",
    "print(a)\n",
    "a = torch.rand(shape)\n",
    "print(a)\n",
    "a = torch.zeros(shape)\n",
    "print(a)\n",
    "a = torch.arange(6,10)\n",
    "print(a,'\\n')\n",
    "a = torch.empty(shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# tensor属性\n",
    "print(a.shape)\n",
    "print(a.dtype)\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy slicing and indexing\n",
    "a = torch.ones(4,4)\n",
    "a[1,:] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# concatenate\n",
    "b = torch.cat([a,a])\n",
    "print(b)\n",
    "b = torch.cat([a,a],axis=1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [4, 5]]) \n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[ 2,  6],\n",
      "        [12, 20]])\n",
      "tensor([[11, 16],\n",
      "        [19, 28]])\n",
      "tensor([[2, 4],\n",
      "        [3, 5]])\n"
     ]
    }
   ],
   "source": [
    "# matrix calculations\n",
    "\n",
    "# multiply\n",
    "\n",
    "a = torch.arange(2,6).reshape(2,2)\n",
    "b = torch.arange(1,5).reshape(2,2)\n",
    "print(a,f'\\n{b}')\n",
    "print(a*b)\n",
    "print(a@b)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[2 3]\n",
      " [4 5]]\n",
      "tensor([[3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# to numpy\n",
    "\n",
    "c = b.numpy()\n",
    "\n",
    "print(c)\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# change tensor is to change numpy array at meantime\n",
    "\n",
    "b += 1\n",
    "\n",
    "print(c)\n",
    "\n",
    "# change numpy array is also to change tensor at meantime !!!!!!!\n",
    "c += 1\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.) tensor(True)\n",
      "tensor(2.) tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Demo 1\n",
    "\n",
    "x = torch.tensor(1., requires_grad = True)\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "b = torch.tensor(4., requires_grad = True)\n",
    "\n",
    "y = w * x ** 2 + b * x + b\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad, x.grad == 2*w*x+b)   # 输出对于x的导数\n",
    "print(b.grad, b.grad == x + 1)     # 输出对于bias的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.backward()`函数的意义：\n",
    "\n",
    "用于计算当前tensor的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求范数:\n",
    "`torch.linalg.norm(input, ord=None, dim=None, keepdim=False, *, out=None, dtype=None) `\n",
    "\n",
    "### ord 参数：\n",
    "\n",
    "一范数：$|x_1|+|x_2|+\\cdots+|x_n|$\n",
    "\n",
    "二范数: $\\sqrt{x_1^2+x_2^2+\\cdots+x_n^2}$\n",
    "\n",
    "无穷范数：$max\\{|x_1|,|x_2|,...,|x_n|\\}$\n",
    "\n",
    "### axis：处理类型\n",
    "\n",
    "axis=1表示按行向量处理，求多个行向量的范数\n",
    "\n",
    "axis=0表示按列向量处理，求多个列向量的范数\n",
    "\n",
    "axis=None表示矩阵范数。\n",
    "\n",
    "### keepdim：是否保持矩阵的二维特性\n",
    "\n",
    "True表示保持矩阵的二维特性，False相反"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(7.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-3,4],dtype = torch.float)\n",
    "z = torch.linalg.norm(x)  # 默认为二范数\n",
    "print(z)\n",
    "z = torch.linalg.norm(x,1) # 一范数\n",
    "print(z)\n",
    "z = torch.linalg.norm(x,float('inf')) # 一范数\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵变换reshape\n",
    "\n",
    "在torch里通常使用view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "x.view(5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "x.grad.zero_() 防止计算的grad叠加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "step_size = 0.25\n",
    "f = lambda x:2*x+3\n",
    "\n",
    "print('iter,\\tx,\\tf(x),\\tf\\'(x),\\tf\\'(x) pytorch')\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward() # compute the gradient\n",
    "    \n",
    "    print('{},\\t{:.3f},\\t{:.3f},\\t{:.3f},\\t{:.3f}'.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))\n",
    "    \n",
    "    x.data = x.data - step_size * x.grad # perform a GD update step\n",
    "    \n",
    "    # We need to zero the grad variable since the backward()\n",
    "    # call accumulates the gradients in .grad instead of overwriting.\n",
    "    # The detach_() is for efficiency. You do not need to worry too much about it.\n",
    "    x.grad.detach_()  # 提高效率\n",
    "    x.grad.zero_()    # 防止梯度叠加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "\n",
    "### Linear module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "dim_input = 3\n",
    "dim_output = 4\n",
    "\n",
    "linear_model = nn.Linear(dim_input, dim_output)  # 设置输入层和输出层的维度\n",
    "x = torch.arange(6).view(2,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, groups = 1)`\n",
    "\n",
    "* `in_channels`: 输入通道。 什么是通道：https://blog.csdn.net/sscc_learning/article/details/79814146\n",
    "* `out_channels`: 输出通道。一般取决于卷积核的数量。\n",
    "* `kernel_size`: 卷积核的大小，当卷积是方形的时候为整数边长，不是方形要输入元组表示高和宽。\n",
    "* `stride`: 卷积核移动的步长\n",
    "* `padding`: 对原图像卷积的填充。\n",
    "* `dilation`: 设置取出输入图像的间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization 标准化\n",
    "\n",
    "`torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)`\n",
    "\n",
    "可以设置momentum\n",
    "\n",
    "```python\n",
    ">>> # With Learnable Parameters\n",
    ">>> m = nn.BatchNorm2d(100)\n",
    ">>> # Without Learnable Parameters\n",
    ">>> m = nn.BatchNorm2d(100, affine=False)\n",
    ">>> input = torch.randn(20, 100, 35, 45)\n",
    ">>> output = m(input)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.utils.data\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "`torch.utils.data.Dataset`\n",
    "\n",
    "Dataset分为两种：1. map-style dataset 2. iterable-style dataset\n",
    "\n",
    "本程序中使用的是map-style dataset\n",
    "\n",
    "映射形式的数据集的类需要实现\\_\\_getitem__() 和 \\_\\_len__()函数来通过index进行数据访问和查看数据集的长度。\n",
    "\n",
    "For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk.\n",
    "\n",
    "\n",
    "`torch.utils.data.DataLoader` 是pytorch数据加载中最实用的包，能够以迭代的形式加载数据集。\n",
    "\n",
    "其可以：\n",
    "\n",
    "* 映射以及迭代的方式加载数据集（map-style and iterable-style）\n",
    "* 自定义数据加载方式\n",
    "* 自动分批（batch）\n",
    "* 单线程或者多线程加载数据\n",
    "* …………"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision\n",
    "## transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
